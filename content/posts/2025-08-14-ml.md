---
title: "First steps with machine learning"
date: 2025-08-14T14:15:05+07:00
slug: ml
description: A bit of theory and practical example of the machine learning process.
image: images/ml/big.png
categories:
  - tech
tags: # "feature" tag tops the post
  - AI
  - feature
draft: false
---
"Artificial Intelligence", "Deep Learning", "Machine Learning" (ML) - these buzzwords have been echoing all around us for the past decade. Most of us think we know what they mean. Or... do we?

I started questioning myself, trying to get a clearer understanding of these concepts. I took some notes, played around with ideas, and somehow ended up building a simple machine learning model. So, let me share what I learned along the way.

## Entering the world of Machine Learning
Machines that "think" aren't exactly new. We’ve had devices like calculators for decades. They seem smart, they do math instantly, after all. But the truth is, they're just really fast at following precise instructions. Everything they do is hard-coded and guaranteed: when you add 1 + 1, it will be 2. No doubts, no debate.

This is classic programming: deterministic, rule-based, and predictable.

Now, enter Machine Learning (ML).
Here, the game changes.

On contrary, machine learning has no exact algorithms and outcome is not precise. So why to bother?
Machine learning shines when writing rules and algorithms would be overly complex if not impossible to define. E.g. to distinguish dog from a cat or to detect sarcasm in the sentence.
Instead of giving the machine rules, we give it examples, lots of them, and let it figure out the patterns. We don’t program the model, we teach it to approximate outcome from a given input.

{{< figure src="images/ml/in-out.png" >}}

Let’s go back to the humble calculator. You give it some inputs, two numbers, and it gives you an output using the magical mathematical operation we call addition. Simple, right?

But let’s simplify even more. Instead of dealing with two numbers on input, imagine we take just one number and transform it into another using a bit of mathematical magic. In other words, we apply a function - a rule that maps input to output.

In ML, we define set of inputs and set of expected outputs. Expected output is oftentimes called a `ground truth` or `target`. *In the context of data classification, you can also hear the term `label` which flags the data in input and output.*

Example inputs and outputs:
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # ground truth
```

The learning process is all about figuring out the rule that takes us from `X` to `Y`. Ideally, the model will discover the underlying pattern, for example, that `Y = 2X - 1` and then use that rule to make predictions for new inputs.

This is a trivial case. In the real world, things get messy. We often deal with complex, multi-dimensional inputs and outputs, images, text, sensor data, you name it. But let’s not go there just yet. For now, simple is good.

Let's summarise the core of the Machine Learning:
- Handles complex tasks
- Scales well and handles well new unseen data
- Improves by having new fresh data
- Needs data (instead of explicit rules)
- Probabilistic

## Real world use cases
Machine learning is used across a wide range of fields, from video processing in self-driving cars, to language processing in the tools like ChatGPT, to predictive analytics in online trading. It powers anomaly detection in cybersecurity and fuels generative AI tools like DALL·E and MidJourney.

With all this buzz, it’s easy to assume that machine learning is always part of something big - big data, big clouds, big models.

But ML isn’t just for massive servers and high-end GPUs. It’s also making its way into the world of microcontrollers, wearables for sports and health, and smart home devices, a trend known as TinyML.
These are scenarios where the benefits of machine learning are needed, but resources like processing power, memory, or battery life are limited. In these cases, ML has to go on a diet and get smart without being power-hungry.

## Basic Learning process
To demonstrate the learning process, we will use the same simple data set as before:
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # ground truth
```

We need to find or approximate the "rule" to get from `X` to `Y`. First, we need to choose a `learning model`, which is a mathematical structure that defines how inputs are mapped to the outputs.

In this simple case, let’s assume our best choice is a `linear regression model` which is represented by a linear function `Y=w * X  + b`.

Here, `w` is called the weight, and `b` is the bias. Together, they define the line that our model uses to approximate the relationship between input and output.

There are much more complex learing models, than regression models, mainly:
- Neural networks - a network of many parameters (weights, biases).
- Decision trees - tree of rules.


Let’s stick with our simple linear regression model. The big question now is: how do we figure out the right values for `w` (weight) and `b` (bias) to make our function work?

The answer?
**We guess.**

Then we check how good the guess was. Then we guess again, but better this time. We repeat this process in a loop, gradually improving our guesses until the model is accurate enough.

In other words, we let the model learn by adjusting `w` and `b` step by step, aiming to reduce the error between its predictions and the actual values. This accuracy is usually measured with some kind of error metric - basically a score telling us how "bad" our guesses still are.

{{< figure src="images/ml/guess.png" >}}

The [following Python code](https://github.com/jkosik/jkosik.github.io/blob/main/_extra/ml/1_ml-manual.py) tries a single manual guess of `w` and `b`. Our first guess is: `w = 3` and `b = 1` and we will calculate also how far we are from the ground truth, which represents the `loss`.

```
import math
import matplotlib.pyplot as plt

# Training data (optimal outcome: y=2x-1)
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # ground truth

# Guess the model (Y=w * X + b)
w = 3
b = 1

# Make a prediction
y_pred = []

for x_val in x:
    y_guess = x_val * w + b
    y_pred.append(y_guess)

print("Ground truth:", y)
print("Prediction:", y_pred)

# Calculate individual losses and overall loss using RMSE (Root Mean Squared Error).
individual_losses = []
print("\nIndividual Losses (Squared Errors):")
for i in range(len(x)):
    error = y_pred[i] - y[i]
    squared_error = error ** 2
    individual_losses.append(squared_error)

print(individual_losses)

# Calculate overall loss (Root Mean Squared Error)
mse = sum(individual_losses) / len(individual_losses)
rmse = math.sqrt(mse)
print(f"\nOverall Loss (Root Mean Squared Error): {rmse:.2f}")

...snipped...
```

As a result we got an array of predicted (guessed) `Y` values, which are slightly off the ground truth and the difference represent the `loss`:
```
Ground truth: [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
Prediction: [-2.0, 1.0, 4.0, 7.0, 10.0, 13.0]

Individual Losses (Squared Errors):
[1.0, 4.0, 9.0, 16.0, 25.0, 36.0]

Overall Loss (Root Mean Squared Error): 3.89
```

The loss is calculated as Root mean squared error (RMSE). RMSE prevents negative losses cancellling out opposite ones. See [this page](https://www.appliedaicourse.com/blog/root-mean-square-error-rmse/) to know more about the RMSE method. There are also others loss calculation methods - MAE (Mean Absolute Error), MSE (Mean Squared Error), etc.

Loss visualised:
{{< figure src="images/ml/plot1.png" >}}

Our overall loss is 3.89 which is quite far from 0 (full precision). Our first guess was very wrong.

## Impoved guessing by Gradient Descent learning
We can continue in guessing, until the loss is low. It will probably take ages for larger input sets.
We can also start observing how loss evolves based on the changes of weight and bias and make adjustments accordingly in the next guess. This technique is called Gradient Descent. Now it can get a bit complicated...The simplest visual representation is a symetric curve and abstraction of bias:

{{< figure src="images/ml/gradient1.png" >}}

By adjusting weight, the loss should get closer to the minimum loss in a stable pace and over a stable gradient. We need to count few parameters: how loss evolves, how gradient (positional change between previous and new loss) evolves. We also need to decide on optimal steps of weight change in the next guess - so called learning step. Large learning step can overshoot the minimum loss. Small learning step may heavily pro,ong the learning process.

No worries, here comes `Tensorflow` framework to save us!

## Tensorflow
Tensorflow allows flow of the data (from simple scalar, through 1D vectors, 2D matrices, up to multidimensional data) through various computational operations.

Tensorflow support so called GradientTape (in Python `tf.GradientTape` class) which runs Gradient Descent learning for us.

Code for running Gradient Descent learning in Python:
```
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Define our initial guess
INITIAL_W = 10.0
INITIAL_B = 10.0

# Define our simple regression model
class Model(object):
    def __init__(self):
        # Initialize the weights
        self.w = tf.Variable(INITIAL_W)
        self.b = tf.Variable(INITIAL_B)

    def __call__(self, x):
        return self.w * x + self.b

# Define our loss function
def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))


# Define our training procedure
def train(model, inputs, outputs, learning_rate):
    with tf.GradientTape() as t:
        # loss function
        current_loss = loss(model(inputs), outputs)

        # Here is where you differentiate loss function w.r.t model parameters
        dw, db = t.gradient(current_loss, [model.w, model.b])  # dloss/dw, dloss/db

        # And here is where you update the model parameters based on the learning rate chosen
        model.w.assign_sub(learning_rate * dw)  # model.w = model.w - learning_rate*dw
        model.b.assign_sub(learning_rate * db)  # model.b = model.b - learning_rate*db
    return current_loss


# Define our input data and learning rate
xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]
ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
LEARNING_RATE = 0.14   #0.09

# Instantiate our model
model = Model()

# Collect the history of w-values and b-values to plot later
list_w, list_b = [], []
epochs = 50
losses = []

for epoch in range(epochs):
    list_w.append(model.w.numpy())
    list_b.append(model.b.numpy())
    current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)
    losses.append(current_loss)
    print('Epoch %2d: w=%.2f b=%.2f, loss=%.2f' %
          (epoch, list_w[-1], list_b[-1], current_loss))
```

Command output shows, how each guess iteration (called Epoch) and adjustment of weight and bias lead to the loss decrease:
```
Epoch  0: w=10.00 b=10.00, loss=715.67
Epoch  1: w=-6.19 b=3.56, loss=255.55
Epoch  2: w=3.74 b=5.72, loss=96.13
Epoch  3: w=-1.60 b=3.11, loss=39.55
Epoch  4: w=1.88 b=3.47, loss=18.51
Epoch  5: w=0.17 b=2.27, loss=10.01
Epoch  6: w=1.44 b=2.12, loss=6.13
Epoch  7: w=0.94 b=1.48, loss=4.08
...snipped...
Epoch 24: w=1.94 b=-0.82, loss=0.02
Epoch 25: w=1.95 b=-0.85, loss=0.01
Epoch 26: w=1.96 b=-0.87, loss=0.01
Epoch 27: w=1.96 b=-0.89, loss=0.01
Epoch 28: w=1.97 b=-0.90, loss=0.01
Epoch 29: w=1.97 b=-0.92, loss=0.00
...snipped...
Epoch 49: w=2.00 b=-1.00, loss=0.00
```

Our loss is 0.00, if weight is 2.00 and bias -1.00. We reached 100% accuracy. Which builds a learned rule `Y=2X-1`. 🎊

Easy, right? But in real world it is much more complex. We can have not 1-demensional input data, but 100-dimensional. Already at 3 dimensions gradient descent looks significantly more complicated:
{{< figure src="images/ml/gradient2.png" >}}



## Neural networks
Regression models are now explained and we have a good foothold to make a next step - Neural networks.
Neural networks is a composition of computational neurons which together solve operations, to find relation between our `X`-es and `Y`-s.
Machine learning using neural networks is also called deep machine learning.

What in `neuron`?
Neuron is a methaphorical term for mathematic function, e.g. `Y=(w * X) + ( ) + ... + ( ) ​+ b`. The longer the polynomial more neurons the network has. Terms inside the polynomial contain weights and inputs. Each layer is the input for thext layer.
Weights are associated with connection between the previous and next layer - more inputs and neurons, more links between them, thus more wights.
Biases are associated with the number of neurons in the particular layer - each neuron introduces certain bias.

The most simple neural networks contains single input, single neuron and single output:

{{< figure src="images/ml/neuron1.png" >}}

This setups results into `Y=(w * X) ​+ b` function.
We will use again Gradient descent loss calculation.

Tensorflow offers abstraction API `keras` for building and training ML models.
```
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Define the model
model = keras.Sequential([
    keras.layers.Dense(units=1, input_shape=[1]) # 1 neuron, 1 input
])

# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error') # stochastic gradient descent (sgd) and measure the loss as MSE

# Training data (y=2x-1)
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) # input
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) # ground truth output


# Train the model
model.fit(xs, ys, epochs=50) # make a guess, measure the loss, optimise and repeat epoch-times

# Make a prediction (runs the input through the trained model.)
print(model.predict(np.array([10.0]))) # try to predict the output of 10.0
```

The code tries to predict `Y` for `X=10` and result in:
```
...snipped...
Epoch 46/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 0.2438
Epoch 47/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.2388
Epoch 48/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.2339
Epoch 49/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - loss: 0.2291
Epoch 50/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: 0.2244
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step
[[17.614405]]
```

Linear regression model (`X=2X-1`) produced the result `19`. Our minimalistic neural network responds with `17.614405`. In this case neural network was outperformed by simpler regression learning model. However regresion model will heavily fail on more complex input data sets.

### Improving the neural network
Neural network will perform better, if we increase the number of epochs (guessing iterations), adjust loss calculation or include more neurons.

The following diagram contains two layers and three neurons in total. The same input `X` is passed through to two neurons in the first layer. First layer neurons forward their results to the second layer where a single neuron executes the final loss detection and calculation and provides the final output `Y`.

{{< figure src="images/ml/neuron2.png" >}}

On the code level we will change the learning model definition as follows:
```
# Define the model
layer1 = keras.layers.Dense(units=2, input_shape=[1]) # input_shape is defined only for the first layer
layer2 = keras.layers.Dense(units=1) # automatically gets inputs from the previous layer

model = keras.Sequential([layer1, layer2]) # 2 layers. 2 neurons in L1, 1 neuron in L2
```

...and the result will be more precise: `18.252928`.

How does our original single-neuron function `Y=(w * X) ​+ b` change in multi-neuron network? Each neuron guesses own weight and bias and the output is passed as an input for the next neuron when new weight and new bias is applied, resulting in `Y=(w3 * N1) + (w4 * N2) + b3`:

{{< figure src="images/ml/neuron3.png" >}}

Now we can introduce one more term - `tensor`. Tensor is a naming for all inputs, outputs and weights. I.e. the data structures of various dimensions which float across the neural network. Sometimes you may hear also a term model `parameters` which are subset of tensors. Our examples have few tens of parameters. Meta Llama or ChatGPT models have hundreds of millions or event trillion+ parameters.

## Complex neural networks
In the real world, neural networks can be much more complex and characterised by:
- many layers
- many neurons in each layer
- neurons can talk to to themselves or neurons in the same layer
- neuron talk to a subset of neurons in the previous layer
- added "activation" functions
- `X` and `Y` can be multidimensional
- and many more

The final computation will be a set of inter-related complex functions with inputs and outputs flow through the network in a convoluted way.

All depends how which use case we are addressing. So far, this blog assumed a standard feedforward network (Dense/Sequential in Keras) where each neuron talks to all neurons in the next layer - that is why "dense", of fully connected network. Keras offers much more than that - [tens of layer types](https://keras.io/api/layers) + custom layers.

## Training data and data classification
ML use cases require strong training data - set of inputs producing expected outputs (called as we know "ground truth" or "labels").
Once training finishes, we still need to validate and test. It is recommended to split the training data to those used only for training and those that will be used for validation only. The reason is, that we want to be sure that trained model can cope properly with previously unseen data, generalises well and does not `overfit` (memorises only samples it was trained on). Overfitting can be result of weak data set, but also result of overtraining the model with excessive amount of epochs.
Validation or test data introduce another dimension into the training process. Besides `loss`, we now calculate also `accuracy`.


Preparing properly normalised data sets is a topic by itself. Luckily `tensorflow` provides multiple [training-ready datasets](https://www.tensorflow.org/datasets/catalog/), which can be loaded directly using the `keras` API we have used so far.

We can load text, sound, images, and so on. Each data sets have in common, that they contain `data` and `labels`. Data can be encoded representation of mp3, written characters, or other inputs.
Label is human a readable name of the data, which allows data `classification`. E.g. we play various samples of the word `yes` and all samples are labeled by `yes`, i.e. tell the training model, that all these samples belong to the same category, or class.

This allows something what we call data `classification` - we can say, that these data represent various rock songs.

### Training on specific data types (image)
Let's see on a high-level how to train a model to detect an image of a dog.

First we need to normalise the data, apply various filters over the image to map relation between individual pixels. You also need to extract key features, e.g. when training a dog recognition, you want to extract separately legs, ears, eyes or body torso to help the model understand what characterise a "dog".
You may also need to augemnt yout training data by applying special function over the existing training data set - e.g. skew or rotate the object to allow the model to generalise as much as possible, rather than overfit.

Various machine learning functions, e.g. [convolutions](https://en.wikipedia.org/wiki/Convolutional_neural_network) may represent dog and dog parts as follows:

{{< figure src="images/ml/dog-convolutions.png" >}}

This kind of decomposition generates various characteristics of a dog, e.g.:
- dog eyes are circles
- ears are always above eyes and triangular
- nose is always below eyes
- four legs support the body
- ...and many more...

When the model is trained properly, any dog image should fit in and will be properly classified. The prediction may look as follows:

{{< figure src="images/ml/dog-detection.png" >}}

## Keyword spotting
Now we enough about ML basics, guessing and measuring training accuracy, training input data, data classification and prediction. We can implement our knowledge into a real-world project - keyword spotting. Keyword spotting is used in many home devices. Think of Alexa or Siri. You say "Hey Google" or "Hey Siri" and the machine responds, because some keywords were spotted. Keywords were spotting by a so-called `TinyML` where minimalistic single-purpose model runs on a microcontroller and activates further cascaded process towards the cloud:

1. User says "Hey Google".
2. TinyML model with very low latency recognises a keyword.
3. Microcontroller records the subsequent words.
4. Subsequent words are passed to the cloud for more heavy processing in a full speech recognition model.
5. User is provided with a full response.

Keyword spotting may sound as a trivial task, but is also very complex. Similarly to image detection, also sound waves must be properly processed and decomposed to train the model well.
It is not enough to record the sound samples and measure amplitudes or overal shape on the sound wave. Even opposite words like `yes` or `no` may look surprisingly similar and would not reliable distinguish completely different words.

{{< figure src="images/ml/yes-no.png" >}}

When training on images, we can apply various convolution layers in our neural network. For sound wave the techniques are just different - e.g. [Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform) being one of many.
Sound waves are decomposed and emphasise unique characteristics and sound features:

{{< figure src="images/ml/yes-no-features.png" >}}

Even minimalistic [Micro Speech](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech) model has 49 spectrographic features, each feature consisting of 40 channels of data.

Anytime Alexa hears a new sound, it runs the input through various transformations in the neural network to classify the words properly.

## Rule your laptop by voice commands.
To avoid reinventing a wheel (and mainly save hours and hours of data preapration or model tuning), we can re-use what smart people already done.

- No need to record own samples, use e.g. [Google's Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) dataset: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz or http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip and get tens of thousands voice samples with few tens of english words.

- No need to train own speech commands models. You can get hundreds of them from [Hugginface](https://huggingface.co/models?search=speech%20command). However you can [train by yourself](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/README.md) as well.

### Using Pre-trained Hugging Face Models
Let's demonstrate how simple it is to use a pre-trained keyword spotting model. The [wav2vec2-base-ft-keyword-spotting](https://huggingface.co/anton-l/wav2vec2-base-ft-keyword-spotting) model achieves 98.26% accuracy and can recognize 12 different keywords including "yes", "no", "up", "down", "left", "right", "on", "off", "stop", "go", plus silence and unknown categories.

```python
import torch
from transformers import Wav2Vec2FeatureExtractor, Wav2Vec2ForSequenceClassification
import librosa

# Load the pre-trained model
model_name = "anton-l/wav2vec2-base-ft-keyword-spotting"
feature_extractor = Wav2Vec2FeatureExtractor.from_pretrained(model_name)
model = Wav2Vec2ForSequenceClassification.from_pretrained(model_name)

# Load and process audio
audio, sr = librosa.load("your_audio.wav", sr=16000)
inputs = feature_extractor(audio, sampling_rate=16000, return_tensors="pt")

# Make prediction
with torch.no_grad():
    outputs = model(**inputs)
    predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
    predicted_class = predictions.argmax().item()
    confidence = predictions.max().item()

# Get the label
id2label = model.config.id2label
predicted_word = id2label[predicted_class]
print(f"Predicted: {predicted_word} (confidence: {confidence:.3f})")
```

When tested on our yes/no samples, this model achieved **100% accuracy** with confidence levels above 90% for all samples. The model correctly distinguished between "yes" and "no" keywords with very high reliability.

The beauty of using pre-trained models is that complex audio preprocessing (like Fourier transforms and feature extraction) is already handled internally. The model was trained on thousands of samples and can immediately work with your audio files.




Raw WAV data → wav_data placeholder → DecodeWav → AudioSpectrogram → AudioMicrofrontend → CNN → Output