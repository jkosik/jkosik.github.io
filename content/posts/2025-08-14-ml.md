---
title: "First steps with machine learning"
date: 2025-08-14T14:15:05+07:00
slug: ml
description: A bit of theory and practical example to the machine learning process.
image: images/ml/big.png
categories:
  - tech
tags: # "feature" tag tops the post
  - AI
  - feature
draft: true
---
"Artificial intelligence", "Deep learning", "Machine learning"(ML) and other buzzwords are resonanting around us this decade. We all more or less know what they mean. Or do we? I started to question myself to better understand the topic and write down the notes and ended up with building a simple machine-learned model. Let me share my journey with you.

## Entering machine learning
There are already machines around us which looks like thinking and reasoning, like simple calculator. Well yes, but all they know is preprogrammed in the machine directly. Calculator has exact knowledge, that adding 1 to 1 results in 2 with guaranteed correctness. This is ensured by generalised algorithms.

On contrary, machine learning has no exact algorithms and outcome is not precise. So why to bother? 
Machine learning shines when rules and algorithms would be overly complex if not impossible to define. E.g. to distinguish dog from cat or to detect sarcasm in teh sentence. That is why we teach the model to approximate outcome from a given input. 

{{< figure src="images/ml/in-out.png" >}}


For example, we define set of inputs and set of expected outputs. Expected output is oftentimes called `label`, `ground truth` or `target`.
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # label / ground truth
```

Learning process tries to find the rule how to get from `X` to `Y`. In ideal case, the learning process will derive to the conclusion, that `X=2X-1` and use it for any input. This is extremely trivial case, in real world we have complex multi-dimensional inputs and outputs, but let's keep it simple for now.


Let's summarise the core of Machine learning:
- Handles complex tasks
- Scales well and handles well new unseen tasks
- Improves by having new fresh data
- Needs data (instead of explicit rules)
- Probabilistic

## Real world use cases
Machine learning can be used in wide areas, ranging from video processing (self-driving cars), language processing (ChatGPT), predictive analysis (online trading), anomaly detection (cybersecurity), generative AI of all kinds (DALL·E, MidJourney). One can think that ML and related models always represent something "big". 

However ML takes its place also in the world of microcontrolers, wearables for sport and health or home devices - so called TinyML. These are use cases where machine learning properties are required, but processor, memory or power consumption are limited.

## Basic Learning process
We will use the same simple data set:
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # label / ground truth
```

We need to find or approximate the "rule" to get from `X` to `Y`. To do so, we need to select `learning model`, which is a mathematical representation how inputs are mapped to outputs. For this case the most suitable learning model is `linear regression model` which is a in mathematics represented by a linear function, in this case `Y=w * X  + b`. 
`w` is called `weight` and `b` is `bias`. Both have their role in calculating the output.

There are much more complex learing models, than regression models, mainly:
- Neural network model → a network of many parameters (weights, biases).
- Decision tree model → tree of rules.

Let's keep it simple and come back to our `linear regression model`. How to calculate weight and bias, i.e. solve the linear function?
Answer is by **guessing** and improving the guesses in a loop until we reach sufficient accuracy.

{{< figure src="images/ml/guess.png" >}}

The following Python code runs manual single guess to try to find optimal `w` and `b` so that `Y=w * X  + b` makes the most reliable outcome.
Our first guess is: `w = 3` and `b = 1` and we will calculate also how far we are from the ground truth, which represents `loss`.
```
import math
import matplotlib.pyplot as plt

# Training data (optimal outcome: y=2x-1)
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # ground truth output

# Guess the model (Y=w * X + b)
w = 3
b = 1

# Make a prediction
y_pred = []

for x_val in x:
    y_guess = x_val * w + b
    y_pred.append(y_guess)

print("Ground truth:", y)
print("Prediction:", y_pred)

# Calculate individual losses and overall loss using RMSE (Root Mean Squared Error).
individual_losses = []
print("\nIndividual Losses (Squared Errors):")
for i in range(len(x)):
    error = y_pred[i] - y[i]
    squared_error = error ** 2
    individual_losses.append(squared_error)

print(individual_losses)

# Calculate overall loss (Root Mean Squared Error)
mse = sum(individual_losses) / len(individual_losses)
rmse = math.sqrt(mse)
print(f"\nOverall Loss (Root Mean Squared Error): {rmse:.2f}")

...snipped...
```

As a result we got an array of predicted (guessed) `Y` values, which are slightly off the ground truth and the difference represent the `loss`:
```
Ground truth: [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
Prediction: [-2.0, 1.0, 4.0, 7.0, 10.0, 13.0]

Individual Losses (Squared Errors):
[1.0, 4.0, 9.0, 16.0, 25.0, 36.0]

Overall Loss (Root Mean Squared Error): 3.89
```

The loss is calculated as Root mean squared error (RMSE). RMSE prevents negative losses cancellling out opposite ones. See [this page](https://www.appliedaicourse.com/blog/root-mean-square-error-rmse/) to know more about the RMSE method. There are also others loss calculation methods - MAE (Mean Absolute Error), MSE (Mean Squared Error), etc.

Loss visualised:
{{< figure src="images/ml/plot1.png" >}}

Our overall loss is 3.89 which is quite far from 0. Our guess was wrong.

## Impoved guessing by Gradient Descent learning
We can continue in guessing, until the loss is low. It will probably take ages for larger input sets. 
We can also start observing how loss evolves based on the changes of weight and bias and make adjustments accrodingly in the next guess. This technique is called Gradient Descent. Now it can get a bit complicated...The simplest visual representation is a symetric curve and abstraction of bias:

{{< figure src="images/ml/gradient1.png" >}}

By adjusting weight, the loss should get closer to the minimum loss in a stable pace and over a stable gradient. We need to count few parameters: how loss evolves, how gradient (positional change between previous and new loss) evolves. We also need to decide on optimal steps of weight change in the next guess - so called learning step. Large learning step can overshoot the minimum loss. Small learning step may heavily pro,ong the learning process.

No worries, here comes `Tensorflow` framework to save us! Tensorflow allows flow of the data (from simple scalar, through 1D vectors, 2D matrices, up to multidimensional data) through various computational operations. 
Tensorflow support so called GradientTape (in Python `tf.GradientTape` class) which runs Gradient Descent learning for us.

Code for running Gradient Descent learning in Python:
```
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Define our initial guess
INITIAL_W = 10.0
INITIAL_B = 10.0

# Define our simple regression model
class Model(object):
    def __init__(self):
        # Initialize the weights
        self.w = tf.Variable(INITIAL_W)
        self.b = tf.Variable(INITIAL_B)

    def __call__(self, x):
        return self.w * x + self.b

# Define our loss function
def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))


# Define our training procedure
def train(model, inputs, outputs, learning_rate):
    with tf.GradientTape() as t:
        # loss function
        current_loss = loss(model(inputs), outputs)

        # Here is where you differentiate loss function w.r.t model parameters
        dw, db = t.gradient(current_loss, [model.w, model.b])  # dloss/dw, dloss/db

        # And here is where you update the model parameters based on the learning rate chosen
        model.w.assign_sub(learning_rate * dw)  # model.w = model.w - learning_rate*dw
        model.b.assign_sub(learning_rate * db)  # model.b = model.b - learning_rate*db
    return current_loss


# Define our input data and learning rate
xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]
ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
LEARNING_RATE = 0.14   #0.09

# Instantiate our model
model = Model()

# Collect the history of w-values and b-values to plot later
list_w, list_b = [], []
epochs = 50
losses = []

for epoch in range(epochs):
    list_w.append(model.w.numpy())
    list_b.append(model.b.numpy())
    current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)
    losses.append(current_loss)
    print('Epoch %2d: w=%.2f b=%.2f, loss=%.2f' %
          (epoch, list_w[-1], list_b[-1], current_loss))
```

Command output shows, how each guess iteration (called Epoch) and adjustment of weight and bias lead to the loss decrease:
```
Epoch  0: w=10.00 b=10.00, loss=715.67
Epoch  1: w=-6.19 b=3.56, loss=255.55
Epoch  2: w=3.74 b=5.72, loss=96.13
Epoch  3: w=-1.60 b=3.11, loss=39.55
Epoch  4: w=1.88 b=3.47, loss=18.51
Epoch  5: w=0.17 b=2.27, loss=10.01
Epoch  6: w=1.44 b=2.12, loss=6.13
Epoch  7: w=0.94 b=1.48, loss=4.08
...snipped...
Epoch 24: w=1.94 b=-0.82, loss=0.02
Epoch 25: w=1.95 b=-0.85, loss=0.01
Epoch 26: w=1.96 b=-0.87, loss=0.01
Epoch 27: w=1.96 b=-0.89, loss=0.01
Epoch 28: w=1.97 b=-0.90, loss=0.01
Epoch 29: w=1.97 b=-0.92, loss=0.00
...snipped...
Epoch 49: w=2.00 b=-1.00, loss=0.00
```

Our loss is 0.00, if weight is 2.00 and bias -1.00. We reached 100% accuracy. Which builds a learned rule `Y=2X-1`. 🎊

Easy, right? But in real world it is much more complex. We can have not 1-demensional input data, but 100-dimensional. Already at 3 dimensions gradient descent looks significantly more complicated:
{{< figure src="images/ml/gradient2.png" >}}

## Neural networks
Regression models are now explain and we have a good foothold to make a next step - use Neural networks to find relation between our `X`-es and `Y`-s
Neural networks is a composition of computational neurons. 

What in `neuron`?
Neuron is a methaphorical term for mathematic function, e.g. `Y=w1 * X1 ​+ w2 * ​X2​ + ... + wN * X​N ​+ b`. 
Neurons are characterized also by an `input_shape` - in our 1-dimensional input set it equals 1 and it makes the neuron (function) look as `Y=w * X  ​+ b`.

Neuron network is usually composed of multiple neuron layer containing multiple neurons. In our example we are ok with a one-layer network with 1 neuron inside.


{{< figure src="images/ml/neuron1.png" >}}


We will use again Gradien descent technique for optimised next guesses.

Tensorflow offers abstraction API layer `keras` to build a neuron network and run learning process for us.
```
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Define the model
model = keras.Sequential([
    keras.layers.Dense(units=1, input_shape=[1]) # 1 neuron, 1 input
])

# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error') # stochastic gradient descent (sgd) and measure the loss as MSE

# Training data (y=2x-1)
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) # input
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) # ground truth output


# Train the model
model.fit(xs, ys, epochs=50) # make a guess, measure the loss, optimise and repeat epoch-times

# Make a prediction (runs the input through the trained model.)
print(model.predict(np.array([10.0]))) # try to predict the output of 10.0
```

The code tries to predict `Y` for `X=10` and result in:
```
...snipped...
Epoch 46/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 32ms/step - loss: 0.2438
Epoch 47/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 22ms/step - loss: 0.2388
Epoch 48/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 16ms/step - loss: 0.2339
Epoch 49/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 18ms/step - loss: 0.2291
Epoch 50/50
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 17ms/step - loss: 0.2244
1/1 ━━━━━━━━━━━━━━━━━━━━ 0s 21ms/step
[[17.614405]]
```

By following `X=2X-1` we would get `19`. Our minimalistic neural network responds by `17.614405]`. In this case neural network was outperformed by regression learning model. However regresion model will heavily fail on more complex input data sets. 
Also in this simple case, neural network will perform well, if we increase the number of epochs (guessing iterations) or parametrise default settings of SDG(stochastic gradient descent) used for guess optimisation.

## More real world neuron network
In real world, neuron networks can be much larger, consisted of many layers of many neurons each and also input and output elemnts can vary.

{{< figure src="images/ml/neuron2.png" >}}

There are also various additional functions, which are executed during the activation of neuron networks.



TDDO:
- end of 3.2 - injecting pixels and get probabilities. Each output neuron represents certain digit

Tuning:
- Activation function(s)
- num of layers
- num of neurons
- links..
- learning optimisation - "adam" instead of "gradient descent"
- layer relation
- loss calculation

SDepends on input data

Use data for training and different for validation and test