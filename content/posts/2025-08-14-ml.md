---
title: "First steps with machine learning"
date: 2025-08-14T14:15:05+07:00
slug: ml
description: A bit of theory and practical example of the machine learning process.
image: images/ml/big.png
categories:
  - tech
tags: # "feature" tag tops the post
  - AI
  - feature
draft: true
---
"Artificial intelligence", "Deep learning", "Machine learning"(ML) and other buzzwords are resonanting around us this decade. We all more or less know what they mean. Or do we? I started to question myself to better understand the topic and write down the notes and ended up with building a simple machine-learned model. Let me share my journey with you.

## Entering machine learning
There are already machines around us which looks like thinking and reasoning, like simple calculator. Well yes, but all they know is preprogrammed in the machine directly. Calculator has exact knowledge, that adding 1 to 1 results in 2 with guaranteed correctness. This is ensured by generalised algorithms.

On contrary, machine learning has no exact algorithms and outcome is not precise. So why to bother? 
Machine learning shines when rules and algorithms would be overly complex if not impossible to define. E.g. to distinguish dog from cat or to detect sarcasm in teh sentence. That is why we teach the model to approximate outcome from a given input. 

{{< figure src="images/ml/in-out.png" >}}


For example, we define set of inputs and set of expected outputs. Expected output is oftentimes called `label`, `ground truth` or `target`.
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # label / ground truth
```

Learning process tries to find the rule how to get from `X` to `Y`. In ideal case, the learning process will derive to the conclusion, that `X=2X-1` and use it for any input. This is extremely trivial case, in real world we have complex multi-dimensional inputs and outputs, but let's keep it simple for now.


Let's summarise the core of Machine learning:
- Handles complex tasks
- Scales well and handles well new unseen tasks
- Improves by having new fresh data
- Needs data (instead of explicit rules)
- Probabilistic

## Real world use cases
Machine learning can be used in wide areas, ranging from video processing (self-driving cars), language processing (ChatGPT), predictive analysis (online trading), anomaly detection (cybersecurity), generative AI of all kinds (DALL¬∑E, MidJourney). One can think that ML and related models always represent something "big". 

However ML takes its place also in the world of microcontrolers, wearables for sport and health or home devices - so called TinyML. These are use cases where machine learning properties are required, but processor, memory or power consumption are limited.

## Basic Learning process
We will use the same simple data set:
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # label / ground truth
```

We need to find or approximate the "rule" to get from `X` to `Y`. To do so, we need to select `learning model`, which is a mathematical representation how inputs are mapped to outputs. For this case the most suitable learning model is `linear regression model` which is a in mathematics represented by a linear function, in this case `Y=w * X  + b`. 
`w` is called `weight` and `b` is `bias`. Both have their role in calculating the output.

There are much more complex learing models, than regression models, mainly:
- Neur network model ‚Üí a network of many parameters (weights, biases).
- Decision tree model ‚Üí tree of rules.

Let's keep it simple and come back to our `linear regression model`. How to calculate weight and bias, i.e. solve the linear function?
Answer is by **guessing** and improving the guesses in a loop until we reach sufficient accuracy.

{{< figure src="images/ml/guess.png" >}}

The following Python code runs manual single guess to try to find optimal `w` and `b` so that `Y=w * X  + b` makes the most reliable outcome.
Our first guess is: `w = 3` and `b = 1` and we will calculate also how far we are from the ground truth, which represents `loss`.
```
import math
import matplotlib.pyplot as plt

# Training data (optimal outcome: y=2x-1)
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # ground truth output

# Guess the model (Y=w * X + b)
w = 3
b = 1

# Make a prediction
y_pred = []

for x_val in x:
    y_guess = x_val * w + b
    y_pred.append(y_guess)

print("Ground truth:", y)
print("Prediction:", y_pred)

# Calculate individual losses and overall loss using RMSE (Root Mean Squared Error).
individual_losses = []
print("\nIndividual Losses (Squared Errors):")
for i in range(len(x)):
    error = y_pred[i] - y[i]
    squared_error = error ** 2
    individual_losses.append(squared_error)

print(individual_losses)

# Calculate overall loss (Root Mean Squared Error)
mse = sum(individual_losses) / len(individual_losses)
rmse = math.sqrt(mse)
print(f"\nOverall Loss (Root Mean Squared Error): {rmse:.2f}")

...snipped...
```

As a result we got an array of predicted (guessed) `Y` values, which are slightly off the ground truth and the difference represent the `loss`:
```
Ground truth: [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
Prediction: [-2.0, 1.0, 4.0, 7.0, 10.0, 13.0]

Individual Losses (Squared Errors):
[1.0, 4.0, 9.0, 16.0, 25.0, 36.0]

Overall Loss (Root Mean Squared Error): 3.89
```

The loss is calculated as Root mean squared error (RMSE). RMSE prevents negative losses cancellling out opposite ones. See [this page](https://www.appliedaicourse.com/blog/root-mean-square-error-rmse/) to know more about the RMSE method. There are also others loss calculation methods - MAE (Mean Absolute Error), MSE (Mean Squared Error), etc.

Loss visualised:
{{< figure src="images/ml/plot1.png" >}}

Our overall loss is 3.89 which is quite far from 0. Our guess was wrong.

## Impoved guessing by Gradient Descent learning
We can continue in guessing, until the loss is low. It will probably take ages for larger input sets. 
We can also start observing how loss evolves based on the changes of weight and bias and make adjustments accrodingly in the next guess. This technique is called Gradient Descent. Now it can get a bit complicated...The simplest visual representation is a symetric curve and abstraction of bias:

{{< figure src="images/ml/gradient1.png" >}}

By adjusting weight, the loss should get closer to the minimum loss in a stable pace and over a stable gradient. We need to count few parameters: how loss evolves, how gradient (positional change between previous and new loss) evolves. We also need to decide on optimal steps of weight change in the next guess - so called learning step. Large learning step can overshoot the minimum loss. Small learning step may heavily pro,ong the learning process.

No worries, here comes `Tensorflow` framework to save us! 

### Tensorflow
Tensorflow allows flow of the data (from simple scalar, through 1D vectors, 2D matrices, up to multidimensional data) through various computational operations. 
Tensorflow support so called GradientTape (in Python `tf.GradientTape` class) which runs Gradient Descent learning for us.

Code for running Gradient Descent learning in Python:
```
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Define our initial guess
INITIAL_W = 10.0
INITIAL_B = 10.0

# Define our simple regression model
class Model(object):
    def __init__(self):
        # Initialize the weights
        self.w = tf.Variable(INITIAL_W)
        self.b = tf.Variable(INITIAL_B)

    def __call__(self, x):
        return self.w * x + self.b

# Define our loss function
def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))


# Define our training procedure
def train(model, inputs, outputs, learning_rate):
    with tf.GradientTape() as t:
        # loss function
        current_loss = loss(model(inputs), outputs)

        # Here is where you differentiate loss function w.r.t model parameters
        dw, db = t.gradient(current_loss, [model.w, model.b])  # dloss/dw, dloss/db

        # And here is where you update the model parameters based on the learning rate chosen
        model.w.assign_sub(learning_rate * dw)  # model.w = model.w - learning_rate*dw
        model.b.assign_sub(learning_rate * db)  # model.b = model.b - learning_rate*db
    return current_loss


# Define our input data and learning rate
xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]
ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
LEARNING_RATE = 0.14   #0.09

# Instantiate our model
model = Model()

# Collect the history of w-values and b-values to plot later
list_w, list_b = [], []
epochs = 50
losses = []

for epoch in range(epochs):
    list_w.append(model.w.numpy())
    list_b.append(model.b.numpy())
    current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)
    losses.append(current_loss)
    print('Epoch %2d: w=%.2f b=%.2f, loss=%.2f' %
          (epoch, list_w[-1], list_b[-1], current_loss))
```

Command output shows, how each guess iteration (called Epoch) and adjustment of weight and bias lead to the loss decrease:
```
Epoch  0: w=10.00 b=10.00, loss=715.67
Epoch  1: w=-6.19 b=3.56, loss=255.55
Epoch  2: w=3.74 b=5.72, loss=96.13
Epoch  3: w=-1.60 b=3.11, loss=39.55
Epoch  4: w=1.88 b=3.47, loss=18.51
Epoch  5: w=0.17 b=2.27, loss=10.01
Epoch  6: w=1.44 b=2.12, loss=6.13
Epoch  7: w=0.94 b=1.48, loss=4.08
...snipped...
Epoch 24: w=1.94 b=-0.82, loss=0.02
Epoch 25: w=1.95 b=-0.85, loss=0.01
Epoch 26: w=1.96 b=-0.87, loss=0.01
Epoch 27: w=1.96 b=-0.89, loss=0.01
Epoch 28: w=1.97 b=-0.90, loss=0.01
Epoch 29: w=1.97 b=-0.92, loss=0.00
...snipped...
Epoch 49: w=2.00 b=-1.00, loss=0.00
```

Our loss is 0.00, if weight is 2.00 and bias -1.00. We reached 100% accuracy. Which builds a learned rule `Y=2X-1`. üéä

Easy, right? But in real world it is much more complex. We can have not 1-demensional input data, but 100-dimensional. Already at 3 dimensions gradient descent looks significantly more complicated:
{{< figure src="images/ml/gradient2.png" >}}

## Neural networks
Regression models are now explained and we have a good foothold to make a next step - Neural networks.
Neural networks is a composition of computational neurons which together solve operations, to find relation between our `X`-es and `Y`-s.
Machine learning using neural networks is also called deep machine learning.


What in `neuron`?
Neuron is a methaphorical term for mathematic function, e.g. `Y=(w * X) + ( ) + ... + ( ) ‚Äã+ b`. The longer the polynomial more neurons the network has. Terms inside the polynomial contain weights and inputs. Each layer is the input for thext layer.
Weights are associated with connection between the previous and next layer - more inputs and neurons, more links between them, thus more wights.
Biases are associated with the number of neurons in the particular layer - each neuron introduces certain bias.

The most simple neural networks contains single input, single neuron and single output:

{{< figure src="images/ml/neuron1.png" >}}

This setups results into `Y=(w * X) ‚Äã+ b` function.
We will use again Gradient descent loss calculation.

Tensorflow offers abstraction API `keras` for building and training ML models.
```
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Define the model
model = keras.Sequential([
    keras.layers.Dense(units=1, input_shape=[1]) # 1 neuron, 1 input
])

# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error') # stochastic gradient descent (sgd) and measure the loss as MSE

# Training data (y=2x-1)
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) # input
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) # ground truth output


# Train the model
model.fit(xs, ys, epochs=50) # make a guess, measure the loss, optimise and repeat epoch-times

# Make a prediction (runs the input through the trained model.)
print(model.predict(np.array([10.0]))) # try to predict the output of 10.0
```

The code tries to predict `Y` for `X=10` and result in:
```
...snipped...
Epoch 46/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 32ms/step - loss: 0.2438
Epoch 47/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.2388
Epoch 48/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 16ms/step - loss: 0.2339
Epoch 49/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 18ms/step - loss: 0.2291
Epoch 50/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 17ms/step - loss: 0.2244
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step
[[17.614405]]
```

Linear regression model (`X=2X-1`) produced the result `19`. Our minimalistic neural network responds with `17.614405`. In this case neural network was outperformed by simpler regression learning model. However regresion model will heavily fail on more complex input data sets. 

### Improving the neural network
Neural network will perform better, if we increase the number of epochs (guessing iterations), adjust loss calculation or include more neurons.

The following diagram contains two layers and three neurons in total. The same input `X` is passed through to two neurons in the first layer. First layer neurons forward their results to the second layer where a single neuron executes the final loss detection and calculation and provides the final output `Y`.

{{< figure src="images/ml/neuron2.png" >}}

On the code level we will change the learning model definition as follows:
```
# Define the model
layer1 = keras.layers.Dense(units=2, input_shape=[1]) # input_shape is defined only for the first layer
layer2 = keras.layers.Dense(units=1) # automatically gets inputs from the previous layer

model = keras.Sequential([layer1, layer2]) # 2 layers. 2 neurons in L1, 1 neuron in L2
```

...and the result will be more precise: `18.252928`.

How does our original single-neuron function `Y=(w * X) ‚Äã+ b` change in multi-neuron network? Each neuron guesses own weight and bias and the output is passed as an input for the next neuron when new weight and new bias is applied, resulting in `Y=(w3 * N1) + (w4 * N2) + b3`:

{{< figure src="images/ml/neuron3.png" >}}

## Real-world
In the real world, neural networks can be much more complex and characterised by:
- many layers
- many neurons in each layer
- neurons can talk to to themselves or neurons in the same layer
- neuron talk to a subset of neurons in the previous layer
- added "activation" functions
- `X` and `Y` can be multidimensional
- and many more

The final computation will be a set of inter-related complex functions with inputs and outputs flow through the network in a convoluted way.
All depends how which use case we are addressing. However this blog assumed so far a standard feedforward network (Dense/Sequential in Keras) where each neuron talks to all neurons in the next layer - that is why "dense", of fully connected network.

### Training data
ML use cases require strong training data - set of inputs producing expected outputs (called as we know "ground truth" or "labels").
Once training finishes, we still need to validate and test. It is recommended to split the training data to those used only for training and those that will be used for validation only. The reason is, that we want to be sure that trained model can cope properly with previously unseen data, generalises well and does not overfit (memorises only samples it was trained on). Overfitting can be result of weak data set, but also result of overtraining the model with excessive amount of epochs.
Validation or test data introduce another dimension into the training process. Besides `loss`, we now calculate also `accuracy`.



Preparing properly normalised data sets is a topic by itself. Luckily `tensorflow` provides multiple [training-ready datasets](https://www.tensorflow.org/datasets/catalog/), which can be loaded directly using the `keras` API we have used so far.

We can load text, sound, images, and so on. Each data sets have in common, that they contain `data` and `label`. Data can be encoded representation of mp3 or writtent characters. 
Label is human a readable name of the data - e.g `rock/pop/jazzz` or `a/b/c/d/e...`. This allows something what we call data `classification` - we can say, that these data represent various rock songs.




----

images processing - not jusst flattening pixels, but applying filter on picture first, map relation between surrounding pixels, extract key object features - person's hands, head...to assume image is a person.

augment input data set by augmentation - skew, rotate - to avoid manual creation of new samples => teach model to generalise more.




Model design is much more complex for more complex data.
Dense ‚Äî fully connected layer.

Activation ‚Äî applies an activation function.

Dropout ‚Äî randomly sets input units to 0 during training.

Flatten ‚Äî flattens input.

Reshape ‚Äî reshapes tensor.

Permute ‚Äî permutes input dimensions.

RepeatVector ‚Äî repeats inputs for a fixed number of timesteps.

Lambda ‚Äî wraps custom functions as a layer.


Convolutional Layers

Conv1D, Conv2D, Conv3D ‚Äî 1D/2D/3D convolutions.

Conv2DTranspose, Conv3DTranspose ‚Äî transposed (a.k.a. deconvolutional) conv layers.

SeparableConv1D, SeparableConv2D ‚Äî depthwise separable convolutions.

DepthwiseConv2D ‚Äî depthwise convolution.

Conv1DTranspose (experimental in some versions).


Pooling Layers

MaxPooling1D, MaxPooling2D, MaxPooling3D

AveragePooling1D, AveragePooling2D, AveragePooling3D

GlobalMaxPooling1D, GlobalMaxPooling2D, GlobalMaxPooling3D

GlobalAveragePooling1D, GlobalAveragePooling2D, GlobalAveragePooling3D

Recurrent Layers

SimpleRNN

GRU

LSTM

Bidirectional (wrapper for RNNs)

TimeDistributed (wrapper to apply a layer to each timestep)

Normalization & Regularization

BatchNormalization

LayerNormalization

ActivityRegularization

Embedding & Input

Embedding ‚Äî maps integers to dense vectors.

InputLayer ‚Äî entry point for model input.

Attention & Advanced Layers

Attention

AdditiveAttention

MultiHeadAttention

Merging Layers

Add, Subtract, Multiply, Average, Maximum, Minimum, Concatenate, Dot


+ Custome layers