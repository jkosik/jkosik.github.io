---
title: "First steps with machine learning"
date: 2025-08-14T14:15:05+07:00
slug: ml
description: A bit of theory and practical example of the machine learning process.
image: images/ml/big.png
categories:
  - tech
tags: # "feature" tag tops the post
  - AI
  - feature
draft: true
---
"Artificial intelligence", "Deep learning", "Machine learning"(ML) and other buzzwords are resonanting around us this decade. We all more or less know what they mean. Or do we? I started to question myself to better understand the topic and write down the notes and ended up with building a simple machine-learned model. Let me share my journey with you.

## Entering machine learning
There are already machines around us which looks like thinking and reasoning, like simple calculator. Well yes, but all they know is preprogrammed in the machine directly. Calculator has exact knowledge, that adding 1 to 1 results in 2 with guaranteed correctness. This is ensured by generalised algorithms.

On contrary, machine learning has no exact algorithms and outcome is not precise. So why to bother?
Machine learning shines when rules and algorithms would be overly complex if not impossible to define. E.g. to distinguish dog from cat or to detect sarcasm in teh sentence. That is why we teach the model to approximate outcome from a given input.

{{< figure src="images/ml/in-out.png" >}}


For example, we define set of inputs and set of expected outputs. Expected output is oftentimes called `label`, `ground truth` or `target`.
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # label / ground truth
```

Learning process tries to find the rule how to get from `X` to `Y`. In ideal case, the learning process will derive to the conclusion, that `X=2X-1` and use it for any input. This is extremely trivial case, in real world we have complex multi-dimensional inputs and outputs, but let's keep it simple for now.


Let's summarise the core of Machine learning:
- Handles complex tasks
- Scales well and handles well new unseen tasks
- Improves by having new fresh data
- Needs data (instead of explicit rules)
- Probabilistic

## Real world use cases
Machine learning can be used in wide areas, ranging from video processing (self-driving cars), language processing (ChatGPT), predictive analysis (online trading), anomaly detection (cybersecurity), generative AI of all kinds (DALL¬∑E, MidJourney). One can think that ML and related models always represent something "big".

However ML takes its place also in the world of microcontrolers, wearables for sport and health or home devices - so called TinyML. These are use cases where machine learning properties are required, but processor, memory or power consumption are limited.

## Basic Learning process
We will use the same simple data set:
```
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # label / ground truth
```

We need to find or approximate the "rule" to get from `X` to `Y`. To do so, we need to select `learning model`, which is a mathematical representation how inputs are mapped to outputs. For this case the most suitable learning model is `linear regression model` which is a in mathematics represented by a linear function, in this case `Y=w * X  + b`.
`w` is called `weight` and `b` is `bias`. Both have their role in calculating the output.

There are much more complex learing models, than regression models, mainly:
- Neur network model ‚Üí a network of many parameters (weights, biases).
- Decision tree model ‚Üí tree of rules.

Let's keep it simple and come back to our `linear regression model`. How to calculate weight and bias, i.e. solve the linear function?
Answer is by **guessing** and improving the guesses in a loop until we reach sufficient accuracy - measured as a success rate of the guessing.

{{< figure src="images/ml/guess.png" >}}

The following Python code runs manual single guess to try to find optimal `w` and `b` so that `Y=w * X  + b` makes the most reliable outcome.
Our first guess is: `w = 3` and `b = 1` and we will calculate also how far we are from the ground truth, which represents `loss`.
```
import math
import matplotlib.pyplot as plt

# Training data (optimal outcome: y=2x-1)
x = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0] # input
y = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0] # ground truth output

# Guess the model (Y=w * X + b)
w = 3
b = 1

# Make a prediction
y_pred = []

for x_val in x:
    y_guess = x_val * w + b
    y_pred.append(y_guess)

print("Ground truth:", y)
print("Prediction:", y_pred)

# Calculate individual losses and overall loss using RMSE (Root Mean Squared Error).
individual_losses = []
print("\nIndividual Losses (Squared Errors):")
for i in range(len(x)):
    error = y_pred[i] - y[i]
    squared_error = error ** 2
    individual_losses.append(squared_error)

print(individual_losses)

# Calculate overall loss (Root Mean Squared Error)
mse = sum(individual_losses) / len(individual_losses)
rmse = math.sqrt(mse)
print(f"\nOverall Loss (Root Mean Squared Error): {rmse:.2f}")

...snipped...
```

As a result we got an array of predicted (guessed) `Y` values, which are slightly off the ground truth and the difference represent the `loss`:
```
Ground truth: [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
Prediction: [-2.0, 1.0, 4.0, 7.0, 10.0, 13.0]

Individual Losses (Squared Errors):
[1.0, 4.0, 9.0, 16.0, 25.0, 36.0]

Overall Loss (Root Mean Squared Error): 3.89
```

The loss is calculated as Root mean squared error (RMSE). RMSE prevents negative losses cancellling out opposite ones. See [this page](https://www.appliedaicourse.com/blog/root-mean-square-error-rmse/) to know more about the RMSE method. There are also others loss calculation methods - MAE (Mean Absolute Error), MSE (Mean Squared Error), etc.

Loss visualised:
{{< figure src="images/ml/plot1.png" >}}

Our overall loss is 3.89 which is quite far from 0. Our guess was wrong.

## Impoved guessing by Gradient Descent learning
We can continue in guessing, until the loss is low. It will probably take ages for larger input sets.
We can also start observing how loss evolves based on the changes of weight and bias and make adjustments accordingly in the next guess. This technique is called Gradient Descent. Now it can get a bit complicated...The simplest visual representation is a symetric curve and abstraction of bias:

{{< figure src="images/ml/gradient1.png" >}}

By adjusting weight, the loss should get closer to the minimum loss in a stable pace and over a stable gradient. We need to count few parameters: how loss evolves, how gradient (positional change between previous and new loss) evolves. We also need to decide on optimal steps of weight change in the next guess - so called learning step. Large learning step can overshoot the minimum loss. Small learning step may heavily pro,ong the learning process.

No worries, here comes `Tensorflow` framework to save us!

## Tensorflow
Tensorflow allows flow of the data (from simple scalar, through 1D vectors, 2D matrices, up to multidimensional data) through various computational operations.

Tensorflow support so called GradientTape (in Python `tf.GradientTape` class) which runs Gradient Descent learning for us.

Code for running Gradient Descent learning in Python:
```
import tensorflow as tf
import matplotlib.pyplot as plt
import numpy as np

# Define our initial guess
INITIAL_W = 10.0
INITIAL_B = 10.0

# Define our simple regression model
class Model(object):
    def __init__(self):
        # Initialize the weights
        self.w = tf.Variable(INITIAL_W)
        self.b = tf.Variable(INITIAL_B)

    def __call__(self, x):
        return self.w * x + self.b

# Define our loss function
def loss(predicted_y, target_y):
    return tf.reduce_mean(tf.square(predicted_y - target_y))


# Define our training procedure
def train(model, inputs, outputs, learning_rate):
    with tf.GradientTape() as t:
        # loss function
        current_loss = loss(model(inputs), outputs)

        # Here is where you differentiate loss function w.r.t model parameters
        dw, db = t.gradient(current_loss, [model.w, model.b])  # dloss/dw, dloss/db

        # And here is where you update the model parameters based on the learning rate chosen
        model.w.assign_sub(learning_rate * dw)  # model.w = model.w - learning_rate*dw
        model.b.assign_sub(learning_rate * db)  # model.b = model.b - learning_rate*db
    return current_loss


# Define our input data and learning rate
xs = [-1.0, 0.0, 1.0, 2.0, 3.0, 4.0]
ys = [-3.0, -1.0, 1.0, 3.0, 5.0, 7.0]
LEARNING_RATE = 0.14   #0.09

# Instantiate our model
model = Model()

# Collect the history of w-values and b-values to plot later
list_w, list_b = [], []
epochs = 50
losses = []

for epoch in range(epochs):
    list_w.append(model.w.numpy())
    list_b.append(model.b.numpy())
    current_loss = train(model, xs, ys, learning_rate=LEARNING_RATE)
    losses.append(current_loss)
    print('Epoch %2d: w=%.2f b=%.2f, loss=%.2f' %
          (epoch, list_w[-1], list_b[-1], current_loss))
```

Command output shows, how each guess iteration (called Epoch) and adjustment of weight and bias lead to the loss decrease:
```
Epoch  0: w=10.00 b=10.00, loss=715.67
Epoch  1: w=-6.19 b=3.56, loss=255.55
Epoch  2: w=3.74 b=5.72, loss=96.13
Epoch  3: w=-1.60 b=3.11, loss=39.55
Epoch  4: w=1.88 b=3.47, loss=18.51
Epoch  5: w=0.17 b=2.27, loss=10.01
Epoch  6: w=1.44 b=2.12, loss=6.13
Epoch  7: w=0.94 b=1.48, loss=4.08
...snipped...
Epoch 24: w=1.94 b=-0.82, loss=0.02
Epoch 25: w=1.95 b=-0.85, loss=0.01
Epoch 26: w=1.96 b=-0.87, loss=0.01
Epoch 27: w=1.96 b=-0.89, loss=0.01
Epoch 28: w=1.97 b=-0.90, loss=0.01
Epoch 29: w=1.97 b=-0.92, loss=0.00
...snipped...
Epoch 49: w=2.00 b=-1.00, loss=0.00
```

Our loss is 0.00, if weight is 2.00 and bias -1.00. We reached 100% accuracy. Which builds a learned rule `Y=2X-1`. üéä

Easy, right? But in real world it is much more complex. We can have not 1-demensional input data, but 100-dimensional. Already at 3 dimensions gradient descent looks significantly more complicated:
{{< figure src="images/ml/gradient2.png" >}}



## Neural networks
Regression models are now explained and we have a good foothold to make a next step - Neural networks.
Neural networks is a composition of computational neurons which together solve operations, to find relation between our `X`-es and `Y`-s.
Machine learning using neural networks is also called deep machine learning.

What in `neuron`?
Neuron is a methaphorical term for mathematic function, e.g. `Y=(w * X) + ( ) + ... + ( ) ‚Äã+ b`. The longer the polynomial more neurons the network has. Terms inside the polynomial contain weights and inputs. Each layer is the input for thext layer.
Weights are associated with connection between the previous and next layer - more inputs and neurons, more links between them, thus more wights.
Biases are associated with the number of neurons in the particular layer - each neuron introduces certain bias.

The most simple neural networks contains single input, single neuron and single output:

{{< figure src="images/ml/neuron1.png" >}}

This setups results into `Y=(w * X) ‚Äã+ b` function.
We will use again Gradient descent loss calculation.

Tensorflow offers abstraction API `keras` for building and training ML models.
```
import tensorflow as tf
from tensorflow import keras
import numpy as np

# Define the model
model = keras.Sequential([
    keras.layers.Dense(units=1, input_shape=[1]) # 1 neuron, 1 input
])

# Compile the model
model.compile(optimizer='sgd', loss='mean_squared_error') # stochastic gradient descent (sgd) and measure the loss as MSE

# Training data (y=2x-1)
xs = np.array([-1.0, 0.0, 1.0, 2.0, 3.0, 4.0], dtype=float) # input
ys = np.array([-3.0, -1.0, 1.0, 3.0, 5.0, 7.0], dtype=float) # ground truth output


# Train the model
model.fit(xs, ys, epochs=50) # make a guess, measure the loss, optimise and repeat epoch-times

# Make a prediction (runs the input through the trained model.)
print(model.predict(np.array([10.0]))) # try to predict the output of 10.0
```

The code tries to predict `Y` for `X=10` and result in:
```
...snipped...
Epoch 46/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 32ms/step - loss: 0.2438
Epoch 47/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 22ms/step - loss: 0.2388
Epoch 48/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 16ms/step - loss: 0.2339
Epoch 49/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 18ms/step - loss: 0.2291
Epoch 50/50
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 17ms/step - loss: 0.2244
1/1 ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 0s 21ms/step
[[17.614405]]
```

Linear regression model (`X=2X-1`) produced the result `19`. Our minimalistic neural network responds with `17.614405`. In this case neural network was outperformed by simpler regression learning model. However regresion model will heavily fail on more complex input data sets.

### Improving the neural network
Neural network will perform better, if we increase the number of epochs (guessing iterations), adjust loss calculation or include more neurons.

The following diagram contains two layers and three neurons in total. The same input `X` is passed through to two neurons in the first layer. First layer neurons forward their results to the second layer where a single neuron executes the final loss detection and calculation and provides the final output `Y`.

{{< figure src="images/ml/neuron2.png" >}}

On the code level we will change the learning model definition as follows:
```
# Define the model
layer1 = keras.layers.Dense(units=2, input_shape=[1]) # input_shape is defined only for the first layer
layer2 = keras.layers.Dense(units=1) # automatically gets inputs from the previous layer

model = keras.Sequential([layer1, layer2]) # 2 layers. 2 neurons in L1, 1 neuron in L2
```

...and the result will be more precise: `18.252928`.

How does our original single-neuron function `Y=(w * X) ‚Äã+ b` change in multi-neuron network? Each neuron guesses own weight and bias and the output is passed as an input for the next neuron when new weight and new bias is applied, resulting in `Y=(w3 * N1) + (w4 * N2) + b3`:

{{< figure src="images/ml/neuron3.png" >}}

Now we can introduce one more term - `tensor`. Tensor is a naming for all inputs, outputs and weights. I.e. the data structures of various dimensions which float across the neural network. Sometimes you may hear also a term model `parameters` which are subset of tensors. Our examples have few tens of parameters. Meta Llama or ChatGPT models have hundreds of millions or event trillion+ parameters.

## Complex neural networks
In the real world, neural networks can be much more complex and characterised by:
- many layers
- many neurons in each layer
- neurons can talk to to themselves or neurons in the same layer
- neuron talk to a subset of neurons in the previous layer
- added "activation" functions
- `X` and `Y` can be multidimensional
- and many more

The final computation will be a set of inter-related complex functions with inputs and outputs flow through the network in a convoluted way.

All depends how which use case we are addressing. So far, this blog assumed a standard feedforward network (Dense/Sequential in Keras) where each neuron talks to all neurons in the next layer - that is why "dense", of fully connected network. Keras offers much more than that - [tens of layer types](https://keras.io/api/layers) + custom layers.

## Training data and data classification
ML use cases require strong training data - set of inputs producing expected outputs (called as we know "ground truth" or "labels").
Once training finishes, we still need to validate and test. It is recommended to split the training data to those used only for training and those that will be used for validation only. The reason is, that we want to be sure that trained model can cope properly with previously unseen data, generalises well and does not `overfit` (memorises only samples it was trained on). Overfitting can be result of weak data set, but also result of overtraining the model with excessive amount of epochs.
Validation or test data introduce another dimension into the training process. Besides `loss`, we now calculate also `accuracy`.


Preparing properly normalised data sets is a topic by itself. Luckily `tensorflow` provides multiple [training-ready datasets](https://www.tensorflow.org/datasets/catalog/), which can be loaded directly using the `keras` API we have used so far.

We can load text, sound, images, and so on. Each data sets have in common, that they contain `data` and `labels`. Data can be encoded representation of mp3, written characters, or other inputs.
Label is human a readable name of the data, which allows data `classification`. E.g. we play various samples of the word `yes` and all samples are labeled by `yes`, i.e. tell the training model, that all these samples belong to the same category, or class.

This allows something what we call data `classification` - we can say, that these data represent various rock songs.

### Training on specific data types (image)
Let's see on a high-level how to train a model to detect an image of a dog.

First we need to normalise the data, apply various filters over the image to map relation between individual pixels. You also need to extract key features, e.g. when training a dog recognition, you want to extract separately legs, ears, eyes or body torso to help the model understand what characterise a "dog".
You may also need to augemnt yout training data by applying special function over the existing training data set - e.g. skew or rotate the object to allow the model to generalise as much as possible, rather than overfit.

Various machine learning functions, e.g. [convolutions](https://en.wikipedia.org/wiki/Convolutional_neural_network) may represent dog and dog parts as follows:

{{< figure src="images/ml/dog-convolutions.png" >}}

This kind of decomposition generates various characteristics of a dog, e.g.:
- dog eyes are circles
- ears are always above eyes and triangular
- nose is always below eyes
- four legs support the body
- ...and many more...

When the model is trained properly, any dog image should fit in and will be properly classified. The prediction may look as follows:

{{< figure src="images/ml/dog-detection.png" >}}

## Keyword spotting
Now we enough about ML basics, guessing and measuring training accuracy, training input data, data classification and prediction. We can implement our knowledge into a real-world project - keyword spotting. Keyword spotting is used in many home devices. Think of Alexa or Siri. You say "Hey Google" or "Hey Siri" and the machine responds, because some keywords were spotted. Keywords were spotting by a so-called `TinyML` where minimalistic single-purpose model runs on a microcontroller and activates further cascaded process towards the cloud:

1. User says "Hey Google".
2. TinyML model with very low latency recognises a keyword.
3. Microcontroller records the subsequent words.
4. Subsequent words are passed to the cloud for more heavy processing in a full speech recognition model.
5. User is provided with a full response.

Keyword spotting may sound as a trivial task, but is also very complex. Similarly to image detection, also sound waves must be properly processed and decomposed to train the model well.
It is not enough to record the sound samples and measure amplitudes or overal shape on the sound wave. Even opposite words like `yes` or `no` may look surprisingly similar and would not reliable distinguish completely different words.

{{< figure src="images/ml/yes-no.png" >}}

When training on images, we can apply various convolution layers in our neural network. For sound wave the techniques are just different - e.g. [Fourier transform](https://en.wikipedia.org/wiki/Fourier_transform) being one of many.
Sound waves are decomposed and emphasise unique characteristics and sound features:

{{< figure src="images/ml/yes-no-features.png" >}}

Even minimalistic [Micro Speech](https://github.com/tensorflow/tflite-micro/tree/main/tensorflow/lite/micro/examples/micro_speech) model has 49 spectrographic features, each feature consisting of 40 channels of data.

Anytime Alexa hears a new sound, it runs the input through various transformations in the neural network to classify the words properly.

## Rule your laptop by voice commands.
To avoid reinventing a wheel (and mainly save hours and hours of data preapration or model tuning), we can re-use what smart people already done.

- No need to record own samples, use e.g. [Google's Speech Commands](https://www.tensorflow.org/datasets/catalog/speech_commands) dataset: http://download.tensorflow.org/data/speech_commands_v0.02.tar.gz or http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip and get tens of thousands voice samples with few tens of english words.
- No need to train own speech commands models. You can get hundreds of them from [Hugginface](https://huggingface.co/models?search=speech%20command). However you can [train by yourself](https://github.com/tensorflow/tflite-micro/blob/main/tensorflow/lite/micro/examples/micro_speech/train/README.md) as well.


